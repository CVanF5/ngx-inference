# vLLM backend service for realistic inference testing
# This serves as the AI workload endpoint that EPP will route to
apiVersion: v1
kind: Service
metadata:
  name: vllm-llama3-8b-instruct
  namespace: ngx-inference-test
  labels:
    app: vllm-llama3-8b-instruct
spec:
  selector:
    app: vllm-llama3-8b-instruct
  ports:
    - port: 8000
      targetPort: 8000
      protocol: TCP
      name: http
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama3-8b-instruct
  namespace: ngx-inference-test
  labels:
    app: vllm-llama3-8b-instruct
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-llama3-8b-instruct
  template:
    metadata:
      labels:
        app: vllm-llama3-8b-instruct
        # Label for InferencePool matching
        model-server: vllm
        model-name: llama3-8b-instruct
    spec:
      containers:
        - name: vllm
          image: vllm/vllm-openai:latest
          command:
            - python3
            - -m
            - vllm.entrypoints.openai.api_server
            - --model
            - meta-llama/Meta-Llama-3-8B-Instruct
            - --port
            - "8000"
            - --host
            - "0.0.0.0"
            # Use CPU for testing (remove this for GPU clusters)
            - --dtype
            - float16
            - --max-model-len
            - "2048"
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: HF_TOKEN
              value: "your-huggingface-token-here"
          resources:
            requests:
              memory: "8Gi"
              cpu: "2000m"
            limits:
              memory: "16Gi"
              cpu: "4000m"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 30
