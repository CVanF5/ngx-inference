load_module /usr/lib/nginx/modules/libngx_inference.so;

worker_processes auto;

# Enable debug logging at main level
error_log /dev/stderr debug;

events {
    worker_connections 1024;
    multi_accept on;
    use epoll;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Logging
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /dev/stdout main;
    error_log /dev/stderr debug;

    # Basic settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    server_tokens off;
    
    # Set client body size limits
    client_max_body_size 50m;  # Allow large AI payloads
    client_body_buffer_size 16k;  # Small buffer to force file buffering for large bodies

    # DNS resolver for service discovery
    resolver 127.0.0.11 valid=30s ipv6=off;

    # Test server with ngx-inference module
    server {
        listen 80;
        server_name _;

        # Test endpoint with EPP (Endpoint Picker Processor)
        location /epp-test {
            # Configure the inference module for EPP
            inference_epp on;
            inference_epp_endpoint "mock-epp:9001";  # service name
            inference_epp_timeout_ms 5000;
            
            # Proxy to the chosen upstream (will be determined by EPP)
            # Use the $inference_upstream variable set by the EPP module
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_pass http://$inference_upstream;
        }

        # Test endpoint with BBR (Body-Based Routing) - now implemented directly in module
        location /bbr-test {
            # Configure the inference module for direct BBR processing
            inference_bbr on;
            inference_bbr_max_body_size 10485760; # 10MB limit for testing
            inference_bbr_default_model "test-model"; # Default for testing
            
            # Proxy to the chosen upstream (will be determined by BBR)
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_pass http://echo-server:80;
        }

        # OpenAI-like API endpoint with both EPP and BBR (BBR now direct)
        location /responses {
            # Configure the inference module for EPP (Endpoint Picker Processor)
            inference_epp on;
            inference_epp_endpoint "mock-epp:9001";  # service name
            inference_epp_timeout_ms 5000;
            
            # Configure the inference module for direct BBR processing
            inference_bbr on;
            inference_bbr_max_body_size 52428800; # 50MB limit for AI workloads
            inference_bbr_default_model "gpt-4"; # Default model for production
            
            # Proxy to the chosen upstream (will be determined by EPP)
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_pass http://$inference_upstream;
        }

        # Health check endpoint
        location /health {
            return 200 "OK\n";
            add_header Content-Type text/plain;
        }

        # Root endpoint for basic testing
        location / {
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_pass http://echo-server:80;
        }
    }
}