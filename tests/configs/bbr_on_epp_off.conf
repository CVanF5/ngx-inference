server {
    listen 8081;
    server_name localhost;

    # Global default upstream for all inference failures
    inference_default_upstream "vllm-llama3-8b-instruct.ngx-inference-test.svc.cluster.local:8000";

    # vLLM Chat Completions API (BBR enabled, EPP disabled)
    location /v1/chat/completions {
        inference_bbr on;
        inference_max_body_size 52428800; # 50MB limit for AI workloads
        inference_bbr_header_name X-Gateway-Model-Name;
        inference_bbr_default_model "meta-llama/Llama-3.1-8B-Instruct";

        set $backend "vllm-llama3-8b-instruct.ngx-inference-test.svc.cluster.local:8000";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://$backend;
    }

    # vLLM Models API with BBR only (EPP disabled)
    location /v1/models {
        inference_bbr on;
        inference_max_body_size 52428800; # 50MB limit for AI workloads
        inference_bbr_header_name X-Gateway-Model-Name;
        inference_bbr_default_model "meta-llama/Llama-3.1-8B-Instruct";

        set $backend "vllm-llama3-8b-instruct.ngx-inference-test.svc.cluster.local:8000";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://$backend;
    }

    # BBR enabled, EPP disabled
    location /bbr-test {
        inference_bbr on;
        inference_max_body_size 10485760; # 10MB
        inference_bbr_header_name X-Gateway-Model-Name;
        inference_bbr_default_model "test-model";

        set $backend "vllm-llama3-8b-instruct.ngx-inference-test.svc.cluster.local:8000";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://$backend;
    }

    location /epp-test {
        # EPP disabled - just proxy to vLLM without EPP processing
        set $backend "vllm-llama3-8b-instruct.ngx-inference-test.svc.cluster.local:8000";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://$backend;
    }

    location /responses {
        # BBR enabled, EPP disabled
        inference_bbr on;
        inference_max_body_size 52428800; # 50MB limit for AI workloads
        inference_bbr_default_model "gpt-4";

        set $backend "vllm-llama3-8b-instruct.ngx-inference-test.svc.cluster.local:8000";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://$backend;
    }

    location /health {
        return 200 "OK\n";
        add_header Content-Type text/plain;
    }

    location / {
        set $backend "vllm-llama3-8b-instruct.ngx-inference-test.svc.cluster.local:8000";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://$backend;
    }
}
