server {
    listen 8081;
    server_name localhost;

    # Global default upstream for all inference failures
    inference_default_upstream "vllm-llama3-8b-instruct.ngx-inference-test.svc.cluster.local:8000";

    # vLLM Chat Completions API with BBR and EPP
    location /v1/chat/completions {
        inference_bbr on;
        inference_bbr_max_body_size 52428800; # 50MB limit for AI workloads
        inference_bbr_header_name X-Gateway-Model-Name;
        inference_bbr_default_model "meta-llama/Llama-3.1-8B-Instruct";

        inference_epp on;
        inference_epp_endpoint "vllm-llama3-8b-instruct-epp:9002";
        inference_epp_timeout_ms 5000;
        inference_epp_tls on;
        inference_epp_ca_file /etc/epp-tls/tls.crt;
        inference_epp_header_name "x-gateway-destination-endpoint";

        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://$inference_upstream;
    }

    # vLLM Models API with BBR and EPP
    location /v1/models {
        inference_bbr on;
        inference_bbr_max_body_size 52428800; # 50MB limit for AI workloads
        inference_bbr_header_name X-Gateway-Model-Name;
        inference_bbr_default_model "meta-llama/Llama-3.1-8B-Instruct";

        inference_epp on;
        inference_epp_endpoint "vllm-llama3-8b-instruct-epp:9002";
        inference_epp_timeout_ms 5000;
        inference_epp_tls on;
        inference_epp_ca_file /etc/epp-tls/tls.crt;
        inference_epp_header_name "x-gateway-destination-endpoint";

        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://$inference_upstream;
    }

    # Both BBR and EPP enabled - BBR extracts model, but we still need EPP to set upstream
    location /bbr-test {
        inference_bbr on;
        inference_bbr_max_body_size 10485760; # 10MB
        inference_bbr_header_name X-Gateway-Model-Name;
        inference_bbr_default_model "test-model";

        # BBR alone doesn't set upstream, use vLLM backend
        set $backend "vllm-llama3-8b-instruct.ngx-inference-test.svc.cluster.local:8000";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://$backend;
    }

    location /epp-test {
        inference_epp on;
        inference_epp_endpoint "vllm-llama3-8b-instruct-epp:9002";
        inference_epp_timeout_ms 5000;
        inference_epp_tls on;
        inference_epp_ca_file /etc/epp-tls/tls.crt;
        inference_epp_header_name "x-gateway-destination-endpoint";

        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://$inference_upstream;
    }

    location /responses {
        # Both BBR and EPP enabled
        inference_epp on;
        inference_epp_endpoint "vllm-llama3-8b-instruct-epp:9002";
        inference_epp_timeout_ms 5000;
        inference_epp_tls on;
        inference_epp_ca_file /etc/epp-tls/tls.crt;
        inference_epp_header_name "x-gateway-destination-endpoint";

        inference_bbr on;
        inference_bbr_max_body_size 52428800; # 50MB limit for AI workloads
        inference_bbr_default_model "gpt-4";

        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://$inference_upstream;
    }

    location /health {
        return 200 "OK\n";
        add_header Content-Type text/plain;
    }

    location / {
        set $backend "vllm-llama3-8b-instruct.ngx-inference-test.svc.cluster.local:8000";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://$backend;
    }
}
