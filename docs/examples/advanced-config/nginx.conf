# Advanced NGINX Configuration for ngx-inference module
# This example shows a complete setup with both BBR and EPP features

load_module /usr/lib/nginx/modules/libngx_inference.so;

worker_processes auto;

events {
    worker_connections 1024;
    multi_accept on;
    use epoll;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Enhanced logging with inference variables
    log_format inference '$remote_addr - $remote_user [$time_local] "$request" '
                        '$status $body_bytes_sent "$http_referer" '
                        '"$http_user_agent" "$http_x_forwarded_for" '
                        'upstream="$inference_upstream"';

    access_log /var/log/nginx/access.log inference;
    error_log /var/log/nginx/error.log debug;

    # Performance settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 75s;
    keepalive_requests 100;
    types_hash_max_size 2048;
    server_tokens off;

    # Configure body handling for large AI workloads
    client_max_body_size 100m;
    client_body_buffer_size 64k;
    client_body_timeout 60s;
    client_header_timeout 60s;

    # Upstream pools (replace with your actual backends)
    upstream ai-model-gpt4 {
        server ai-backend-1:8080 weight=3;
        server ai-backend-2:8080 weight=1;
        keepalive 32;
    }

    upstream ai-model-claude {
        server ai-backend-3:8080;
        keepalive 16;
    }

    upstream ai-model-llama {
        server ai-backend-4:8080;
        server ai-backend-5:8080;
        keepalive 16;
    }

    # Main server configuration
    server {
        listen 80;
        server_name api.example.com;

        # OpenAI-compatible API with full inference pipeline
        location /v1/chat/completions {
            # Enable Body-Based Routing to extract model from request
            inference_bbr on;
            inference_max_body_size 104857600; # 100MB
            inference_bbr_header_name X-Gateway-Model-Name;
            inference_bbr_failure_mode_allow off; # Fail closed for production

            # Enable Endpoint Picker Processor for intelligent routing
            inference_epp on;
            inference_epp_endpoint "epp-service:9001";
            inference_epp_timeout_ms 3000;
            inference_epp_header_name X-Inference-Upstream;
            inference_epp_failure_mode_allow off; # Fail closed for production

            # Route to dynamically selected upstream
            proxy_pass http://$inference_upstream;
            proxy_http_version 1.1;
            proxy_set_header Connection "";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # Timeout settings for AI workloads
            proxy_connect_timeout 30s;
            proxy_send_timeout 300s;
            proxy_read_timeout 300s;
        }

        # Embeddings endpoint with BBR only
        location /v1/embeddings {
            inference_bbr on;
            inference_max_body_size 10485760; # 10MB (embeddings typically smaller)
            inference_bbr_header_name X-Gateway-Model-Name;

            # Static routing to embedding-specific backend
            proxy_pass http://embedding-backend:8080;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        # Models endpoint (no body processing needed)
        location /v1/models {
            proxy_pass http://api-backend:8080;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        # Health check with inference module status
        location /health {
            access_log off;
            return 200 "OK\n";
            add_header Content-Type text/plain;
        }

        # Metrics endpoint (if using monitoring)
        location /metrics {
            access_log off;
            allow 10.0.0.0/8;
            allow 172.16.0.0/12;
            allow 192.168.0.0/16;
            deny all;
            proxy_pass http://metrics-backend:8080;
        }
    }

    # SSL/TLS server (production setup)
    server {
        listen 443 ssl http2;
        server_name api.example.com;

        # SSL configuration
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256;
        ssl_prefer_server_ciphers off;

        # Security headers
        add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
        add_header X-Frame-Options DENY always;
        add_header X-Content-Type-Options nosniff always;

        # Same location blocks as HTTP server
        location /v1/chat/completions {
            inference_bbr on;
            inference_max_body_size 104857600;
            inference_bbr_header_name X-Gateway-Model-Name;
            inference_bbr_failure_mode_allow off;

            inference_epp on;
            inference_epp_endpoint "epp-service:9001";
            inference_epp_timeout_ms 3000;
            inference_epp_header_name X-Inference-Upstream;
            inference_epp_failure_mode_allow off;

            proxy_pass http://$inference_upstream;
            proxy_http_version 1.1;
            proxy_set_header Connection "";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            proxy_connect_timeout 30s;
            proxy_send_timeout 300s;
            proxy_read_timeout 300s;
        }

        # Additional locations as needed...
    }
}