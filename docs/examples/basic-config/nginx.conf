# Basic NGINX Configuration for ngx-inference module
# This example shows a simple setup with Body-Based Routing (BBR) only

load_module /usr/lib/nginx/modules/libngx_inference.so;

worker_processes auto;

events {
    worker_connections 1024;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Logging
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log info;

    # Basic settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    server_tokens off;

    # Configure body handling for AI payloads
    client_max_body_size 20m;
    client_body_buffer_size 32k;

    server {
        listen 80;
        server_name localhost;

        # AI API endpoint with Body-Based Routing
        location /v1/chat/completions {
            # Enable BBR to extract model from JSON body
            inference_bbr on;
            inference_max_body_size 20971520; # 20MB
            inference_bbr_header_name X-Gateway-Model-Name;

            # Fail open if BBR encounters errors
            inference_bbr_failure_mode_allow on;

            # Route to your AI backend
            proxy_pass http://your-ai-backend:8080;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Health check
        location /health {
            return 200 "OK\n";
            add_header Content-Type text/plain;
        }
    }
}